{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('NCI_1': conda)"
  },
  "interpreter": {
   "hash": "7b187336c19c9e85c247d6c6e3e3cad294591fae144eb330f64124e413af1302"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Clean Used Car Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import statistics as stat \n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "import logging as logger"
   ]
  },
  {
   "source": [
    "## Load the data & check the data types"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#load data into dataframe for cleaning \n",
    "\n",
    "raw_data = pd.read_csv(\"../data/vehicles.csv\")\n",
    "  "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "source": [
    "raw_data.shape # checking the expected data has arrived"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "raw_data.info() # get the information breakdown of the data"
   ]
  },
  {
   "source": [
    "## Initial Notes\n",
    "1. The data shows a number of features that contain null values.\n",
    "2. There are a number of features that are not required for modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dropping columns no required "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns not required for modelling \n",
    "data_1 = raw_data.drop(columns=['id', 'url','region_url','VIN', 'county', 'lat','long', 'posting_date', 'image_url', 'description', 'model'])"
   ]
  },
  {
   "source": [
    "## Calulate the number of NaN's per columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count up the number of nulls \n",
    "\n",
    "null_check = data_1.isnull().sum()\n",
    "null_check = pd.DataFrame(null_check, columns=[\"Null\"])\n",
    "total = len(raw_data)\n",
    "null_check['%'] = pd.DataFrame(null_check['Null']/total)\n",
    "null_check.sort_values('%',ascending=False)\n"
   ]
  },
  {
   "source": [
    "## Removing small levels of rows that contain NaN's "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the rows that are missing <= 10% of the over data \n",
    "data_2 = data_1.dropna(subset=['year','transmission', 'fuel', 'odometer','title_status'])\n",
    "null_check_2 = data_2.isnull().sum()\n",
    "null_check_2 = pd.DataFrame(null_check_2, columns=[\"Null\"])\n",
    "total = len(raw_data)\n",
    "null_check_2['%'] = pd.DataFrame(null_check_2['Null']/total)\n",
    "null_check_2.sort_values('%',ascending=False)"
   ]
  },
  {
   "source": [
    "# Check the value counts for all the categorical features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the catigorical type of data\n",
    "data_2['size'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['drive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['paint_color'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['manufacturer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['title_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['fuel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['transmission'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['state'].value_counts()"
   ]
  },
  {
   "source": [
    "## Notes \n",
    "1. fuel has 30k records  classed as other. Unable to know or predict the fuel type so these row will be removed\n",
    "2. Transmission as ~62k records classed as other. Unable to know or predict the transamission type so these rows will be removed. \n",
    "3. in the feature title status, the field is used to highlight if the item being sold is "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the rows of data as per the notes above \n",
    "data_3 = data_2[data_2['fuel'] != 'other']\n",
    "data_3 = data_2[data_2['transmission'] != 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_check_3 = data_3.isnull().sum()\n",
    "null_check_3 = pd.DataFrame(null_check_3, columns=[\"Null\"])\n",
    "total = len(raw_data)\n",
    "null_check_3['%'] = pd.DataFrame(null_check_3['Null']/total)\n",
    "null_check_3.sort_values('%',ascending=False)\n"
   ]
  },
  {
   "source": [
    "## Encoding features\n",
    "Preparing each of the features for imputation using KNN.\n",
    "Each feature will require its NaNs rows removed to a seperate dataframe. These will be merged back once the feature is encoded. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperated the data into two dataframes. One contain all rows with no nan and the other rows with nans\n",
    "prep_encoding = data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =list(prep_encoding.columns)\n",
    "remove = ['year','price', 'odometer']\n",
    "for item in remove:\n",
    "    cols.remove(item)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a dict to hold the encoders for each column\n",
    "encoder_dict = {}\n",
    "\n",
    "#loop through the dataframe and extract non-null, encode them and place them back. \n",
    "for name in cols:\n",
    "    #encoder for the column\n",
    "    encoder_dict[name] = OrdinalEncoder()\n",
    "    \n",
    "    #select no nulls \n",
    "    col = prep_encoding[name]\n",
    "    col_notnull = col[col.notnull()]\n",
    "    reshape = col_notnull.values.reshape(-1,1)\n",
    "\n",
    "    #encode non-nills \n",
    "    encoded_vals = encoder_dict[name].fit_transform(reshape)\n",
    "    prep_encoding.loc[col.notnull(), name] = np.squeeze(encoded_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_encoding.head(50)"
   ]
  },
  {
   "source": [
    "# Impute NaNs using KNN "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "#create a scaler to bring all values on the same scale\n",
    "#scale = MinMaxScaler()\n",
    "#scaled_data = pd.DataFrame(scale.fit_transform(prep_encoding), columns=prep_encoding.columns) # new dataframe \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the scaled data using knn \n",
    "# for testing purpose, a sample is created. Please edit to use to the full scaled data dataframe\n",
    "\n",
    "sample = prep_encoding.sample(frac=0.1, replace=False, random_state=1)\n",
    "finaldataset = pd.DataFrame(imputer.fit_transform(sample), columns=prep_encoding.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the dataset fully imputed and scaled. \n",
    "finaldataset\n",
    "#store the data to be accessed in the modeling file \n",
    "%store finaldataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldataset"
   ]
  },
  {
   "source": [
    "# Reverse the scaled data to its normal form"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(scale.inverse_transform(finaldataset), columns=finaldataset.columns)\n",
    "%store readydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop the data and reverse the encoding to create the orginal but imputed dataframe. \n",
    "for col in x:\n",
    "    #encoder for the column\n",
    "   \n",
    "    reshape_col = x[col].values.reshape(-1,1)\n",
    "    x[col] = encoder_dict[col].inverse_transform(reshape_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for all nulls removed\n",
    "x.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}